{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03: CLIP zero-shot prediction\n",
    "In this exercise, you will perform zero-shot prediction using CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from clip import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random seed\n",
    "# SEED = 1 \n",
    "# NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "# NUM_EPOCHS = 30\n",
    "# EVAL_INTERVAL=1\n",
    "# SAVE_DIR = './log'\n",
    "\n",
    "# # Optimizer\n",
    "# LEARNING_RATE = 1e-1\n",
    "# MOMENTUM = 0.9\n",
    "# STEP=5\n",
    "# GAMMA=0.5\n",
    "\n",
    "# CLIP\n",
    "VISUAL_BACKBONE = 'RN50' # RN50, ViT-B/32, ViT-B/16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_cifar10_test = transforms.Compose([\n",
    "    transforms.Resize(size=224),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='/shareddata', train=False,\n",
    "                                       download=True, transform=transform_cifar10_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "dataset_name = 'CIFAR10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model, preprocess = clip.load(name=VISUAL_BACKBONE, device=device, download_root='/shareddata/clip/')\n",
    "model.to(device)\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Prompt Gereration\n",
    "---\n",
    "\n",
    "Please denfine a function named ``prompt_encode`` to encode the text using CLIP text encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a photo of a' # you can try different prompt\n",
    "\n",
    "def prompt_encode(prompt):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prompt (str): the text prefix before the class\n",
    "\n",
    "    Returns:\n",
    "        text_inputs(torch.Tensor)\n",
    "\n",
    "    \"\"\"\n",
    "    ##################### Write your answer here ##################\n",
    "    # 将文本编码为tensor    \n",
    "    text_inputs= torch.cat([clip.tokenize(f\"{prompt} {c}\") for c in class_names]).to(device)\n",
    "    ###############################################################\n",
    "    return text_inputs\n",
    "\n",
    "# 编码文本提示\n",
    "text_inputs = prompt_encode(prompt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Zero-shot inference\n",
    "---\n",
    "\n",
    "Please denfine a function named ``model_inference``. The function is essential for training and evaluating machine learning models using batched data from dataloaders.\n",
    "\n",
    "**To do**: \n",
    "1. Encode the image.\n",
    "2. Encode the text.\n",
    "3. Calculate the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model, image, text_inputs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (torch.nn.Module): 训练好的机器学习模型\n",
    "        image (torch.Tensor): 输入图像张量\n",
    "        text_inputs (torch.Tensor): 输入文本张量\n",
    "\n",
    "    Returns:\n",
    "        logits (torch.Tensor): 模型的预测logits\n",
    "    \"\"\"\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    logits = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Zero-shot accuracy calculation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RN50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the zero-shot performance on CIFAR10 is 55.84%, visual encoder is RN50.\n"
     ]
    }
   ],
   "source": [
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()  \n",
    "    ##################### Write your answer here ##################\n",
    "    correct_predictions = 0\n",
    "    for image, class_id in test_dataloader:\n",
    "        image=image.to(device)\n",
    "        class_id=class_id.to(device)\n",
    "        \n",
    "        logits=model_inference(model,image,text_inputs)\n",
    "        \n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        correct_predictions += (predictions == class_id).sum().item()\n",
    "    \n",
    "    val_acc = correct_predictions / len(test_set)\n",
    "    \n",
    "    ###############################################################\n",
    "\n",
    "    print(f\"the zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIT-B/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the zero-shot performance on CIFAR10 is 85.47%, visual encoder is ViT-B/32.\n"
     ]
    }
   ],
   "source": [
    "VISUAL_BACKBONE = 'ViT-B/32' # RN50, ViT-B/32, ViT-B/16\n",
    "model, preprocess = clip.load(name=VISUAL_BACKBONE, device=device, download_root='/shareddata/clip/')\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()  \n",
    "    ##################### Write your answer here ##################\n",
    "    correct_predictions = 0\n",
    "    for image, class_id in test_dataloader:\n",
    "        image=image.to(device)\n",
    "        class_id=class_id.to(device)\n",
    "        \n",
    "        logits=model_inference(model,image,text_inputs)\n",
    "        \n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        correct_predictions += (predictions == class_id).sum().item()\n",
    "    \n",
    "    val_acc = correct_predictions / len(test_set)\n",
    "    \n",
    "    ###############################################################\n",
    "\n",
    "    print(f\"the zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ViT-B/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the zero-shot performance on CIFAR10 is 88.03%, visual encoder is ViT-B/16.\n"
     ]
    }
   ],
   "source": [
    "VISUAL_BACKBONE = 'ViT-B/16' \n",
    "model, preprocess = clip.load(name=VISUAL_BACKBONE, device=device, download_root='/shareddata/clip/')\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()  \n",
    "    ##################### Write your answer here ##################\n",
    "    correct_predictions = 0\n",
    "    for image, class_id in test_dataloader:\n",
    "        image=image.to(device)\n",
    "        class_id=class_id.to(device)\n",
    "        \n",
    "        logits=model_inference(model,image,text_inputs)\n",
    "        \n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        correct_predictions += (predictions == class_id).sum().item()\n",
    "    \n",
    "    val_acc = correct_predictions / len(test_set)\n",
    "    \n",
    "    ###############################################################\n",
    "\n",
    "    print(f\"the zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
