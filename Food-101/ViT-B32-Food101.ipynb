{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03: CLIP zero-shot prediction\n",
    "In this exercise, you will perform zero-shot prediction using CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "from clip import clip\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random seed\n",
    "SEED = 1 \n",
    "NUM_CLASS = 101\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# CLIP\n",
    "VISUAL_BACKBONE = 'ViT-B/32' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_food_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "test_set = torchvision.datasets.Food101(root='/shareddata', split='test',\n",
    "                                            download=True, transform=transform_food_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                                   shuffle=False, num_workers=128)\n",
    "\n",
    "class_names = ['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad','beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad','cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry','chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder','club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs','donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon','fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari','fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich','grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros','hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese','macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai','paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine','prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa','sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara','spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu','tuna_tartare', 'waffles']\n",
    "  \n",
    "\n",
    "dataset_name= 'Food-101'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model, preprocess = clip.load(name=VISUAL_BACKBONE, device=device, download_root='/shareddata/clip/')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Prompt Gereration\n",
    "---\n",
    "\n",
    "Please denfine a function named ``prompt_encode`` to encode the text using CLIP text encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '' \n",
    "\n",
    "def prompt_encode(prompt):\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"{prompt} {c}\") for c in class_names]).to(device)\n",
    " \n",
    "    return text_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Zero-shot inference\n",
    "---\n",
    "\n",
    "Please denfine a function named ``model_inference``. The function is essential for training and evaluating machine learning models using batched data from dataloaders.\n",
    "\n",
    "**To do**: \n",
    "1. Encode the image.\n",
    "2. Encode the text.\n",
    "3. Calculate the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model, images, text_inputs):\n",
    "\n",
    "    image_features = model.encode_image(images)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    \n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    \n",
    "    similarity = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Zero-shot accuracy calculation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/198\n",
      "Batch 20/198\n",
      "Batch 30/198\n",
      "Batch 40/198\n",
      "Batch 50/198\n",
      "Batch 60/198\n",
      "Batch 70/198\n",
      "Batch 80/198\n",
      "Batch 90/198\n",
      "Batch 100/198\n",
      "Batch 110/198\n",
      "Batch 120/198\n",
      "Batch 130/198\n",
      "Batch 140/198\n",
      "Batch 150/198\n",
      "Batch 160/198\n",
      "Batch 170/198\n",
      "Batch 180/198\n",
      "Batch 190/198\n",
      "The zero-shot performance on Food-101 is 73.75%, visual encoder is ViT-B/32.\n",
      "The average testing loss is 0.9925.\n"
     ]
    }
   ],
   "source": [
    "#正确lable,无其他文本\n",
    "prompt=''\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(test_dataloader, start=1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        text_inputs = prompt_encode(prompt)\n",
    "\n",
    "        similarity = model_inference(model, images, text_inputs)\n",
    "        \n",
    "        criterion_CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion_CE(similarity, labels)\n",
    "        testing_loss.append(loss.item())\n",
    "        \n",
    "        predictions = torch.argmax(similarity, dim=-1)\n",
    "        acc = (predictions == labels).float().mean()\n",
    "        testing_acc.append(acc.item())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(test_dataloader)}\")\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())   \n",
    "\n",
    "    val_loss = np.mean(testing_loss)\n",
    "    val_acc = np.mean(testing_acc)  \n",
    "    \n",
    "print(f\"The zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")\n",
    "print(f\"The average testing loss is {val_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/198\n",
      "Batch 20/198\n",
      "Batch 30/198\n",
      "Batch 40/198\n",
      "Batch 50/198\n",
      "Batch 60/198\n",
      "Batch 70/198\n",
      "Batch 80/198\n",
      "Batch 90/198\n",
      "Batch 100/198\n",
      "Batch 110/198\n",
      "Batch 120/198\n",
      "Batch 130/198\n",
      "Batch 140/198\n",
      "Batch 150/198\n",
      "Batch 160/198\n",
      "Batch 170/198\n",
      "Batch 180/198\n",
      "Batch 190/198\n",
      "The zero-shot performance on Food-101 is 72.62%, visual encoder is ViT-B/32.\n",
      "The average testing loss is 1.0188.\n"
     ]
    }
   ],
   "source": [
    "prompt='asklhafhewufjkfhtjdy'\n",
    "\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(test_dataloader, start=1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        text_inputs = prompt_encode(prompt)\n",
    "\n",
    "        similarity = model_inference(model, images, text_inputs)\n",
    "        \n",
    "        criterion_CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion_CE(similarity, labels)\n",
    "        testing_loss.append(loss.item())\n",
    "        \n",
    "        predictions = torch.argmax(similarity, dim=-1)\n",
    "        acc = (predictions == labels).float().mean()\n",
    "        testing_acc.append(acc.item())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(test_dataloader)}\")\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())   \n",
    "\n",
    "    val_loss = np.mean(testing_loss)\n",
    "    val_acc = np.mean(testing_acc)\n",
    "\n",
    "print(f\"The zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")\n",
    "print(f\"The average testing loss is {val_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/198\n",
      "Batch 20/198\n",
      "Batch 30/198\n",
      "Batch 40/198\n",
      "Batch 50/198\n",
      "Batch 60/198\n",
      "Batch 70/198\n",
      "Batch 80/198\n",
      "Batch 90/198\n",
      "Batch 100/198\n",
      "Batch 110/198\n",
      "Batch 120/198\n",
      "Batch 130/198\n",
      "Batch 140/198\n",
      "Batch 150/198\n",
      "Batch 160/198\n",
      "Batch 170/198\n",
      "Batch 180/198\n",
      "Batch 190/198\n",
      "The zero-shot performance on Food-101 is 44.19%, visual encoder is ViT-B/32.\n",
      "The average testing loss is 3.4982.\n"
     ]
    }
   ],
   "source": [
    "prompt='apple_pie baby_back_ribs baklava beef_carpaccio beef_tartare beet_salad beignets bibimbap bread_pudding breakfast_burrito'\n",
    "\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(test_dataloader, start=1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        text_inputs = prompt_encode(prompt)\n",
    "\n",
    "        similarity = model_inference(model, images, text_inputs)\n",
    "        \n",
    "        criterion_CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion_CE(similarity, labels)\n",
    "        testing_loss.append(loss.item())\n",
    "        \n",
    "        predictions = torch.argmax(similarity, dim=-1)\n",
    "        acc = (predictions == labels).float().mean()\n",
    "        testing_acc.append(acc.item())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(test_dataloader)}\")\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())   \n",
    "\n",
    "    val_loss = np.mean(testing_loss)\n",
    "    val_acc = np.mean(testing_acc)\n",
    "\n",
    "print(f\"The zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")\n",
    "print(f\"The average testing loss is {val_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/198\n",
      "Batch 20/198\n",
      "Batch 30/198\n",
      "Batch 40/198\n",
      "Batch 50/198\n",
      "Batch 60/198\n",
      "Batch 70/198\n",
      "Batch 80/198\n",
      "Batch 90/198\n",
      "Batch 100/198\n",
      "Batch 110/198\n",
      "Batch 120/198\n",
      "Batch 130/198\n",
      "Batch 140/198\n",
      "Batch 150/198\n",
      "Batch 160/198\n",
      "Batch 170/198\n",
      "Batch 180/198\n",
      "Batch 190/198\n",
      "The zero-shot performance on Food-101 is 72.91%, visual encoder is ViT-B/32.\n",
      "The average testing loss is 1.0658.\n"
     ]
    }
   ],
   "source": [
    "prompt='apple_pie'\n",
    "\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(test_dataloader, start=1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        text_inputs = prompt_encode(prompt)\n",
    "\n",
    "        similarity = model_inference(model, images, text_inputs)\n",
    "        \n",
    "        criterion_CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion_CE(similarity, labels)\n",
    "        testing_loss.append(loss.item())\n",
    "        \n",
    "        predictions = torch.argmax(similarity, dim=-1)\n",
    "        acc = (predictions == labels).float().mean()\n",
    "        testing_acc.append(acc.item())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(test_dataloader)}\")\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())   \n",
    "\n",
    "    val_loss = np.mean(testing_loss)\n",
    "    val_acc = np.mean(testing_acc)\n",
    "\n",
    "print(f\"The zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")\n",
    "print(f\"The average testing loss is {val_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/198\n",
      "Batch 20/198\n",
      "Batch 30/198\n",
      "Batch 40/198\n",
      "Batch 50/198\n",
      "Batch 60/198\n",
      "Batch 70/198\n",
      "Batch 80/198\n",
      "Batch 90/198\n",
      "Batch 100/198\n",
      "Batch 110/198\n",
      "Batch 120/198\n",
      "Batch 130/198\n",
      "Batch 140/198\n",
      "Batch 150/198\n",
      "Batch 160/198\n",
      "Batch 170/198\n",
      "Batch 180/198\n",
      "Batch 190/198\n",
      "The zero-shot performance on Food-101 is 59.65%, visual encoder is ViT-B/32.\n",
      "The average testing loss is 1.6918.\n"
     ]
    }
   ],
   "source": [
    "prompt='baklava beef_tartare'\n",
    "\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(test_dataloader, start=1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        text_inputs = prompt_encode(prompt)\n",
    "\n",
    "        similarity = model_inference(model, images, text_inputs)\n",
    "        \n",
    "        criterion_CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion_CE(similarity, labels)\n",
    "        testing_loss.append(loss.item())\n",
    "        \n",
    "        predictions = torch.argmax(similarity, dim=-1)\n",
    "        acc = (predictions == labels).float().mean()\n",
    "        testing_acc.append(acc.item())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(test_dataloader)}\")\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())   \n",
    "\n",
    "    val_loss = np.mean(testing_loss)\n",
    "    val_acc = np.mean(testing_acc)\n",
    "\n",
    "print(f\"The zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")\n",
    "print(f\"The average testing loss is {val_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/198\n",
      "Batch 20/198\n",
      "Batch 30/198\n",
      "Batch 40/198\n",
      "Batch 50/198\n",
      "Batch 60/198\n",
      "Batch 70/198\n",
      "Batch 80/198\n",
      "Batch 90/198\n",
      "Batch 100/198\n",
      "Batch 110/198\n",
      "Batch 120/198\n",
      "Batch 130/198\n",
      "Batch 140/198\n",
      "Batch 150/198\n",
      "Batch 160/198\n",
      "Batch 170/198\n",
      "Batch 180/198\n",
      "Batch 190/198\n",
      "The zero-shot performance on Food-101 is 61.38%, visual encoder is ViT-B/32.\n",
      "The average testing loss is 2.0244.\n"
     ]
    }
   ],
   "source": [
    "prompt='bibimbap bread_pudding breakfast_burrito'\n",
    "\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(test_dataloader, start=1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        text_inputs = prompt_encode(prompt)\n",
    "\n",
    "        similarity = model_inference(model, images, text_inputs)\n",
    "        \n",
    "        criterion_CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion_CE(similarity, labels)\n",
    "        testing_loss.append(loss.item())\n",
    "        \n",
    "        predictions = torch.argmax(similarity, dim=-1)\n",
    "        acc = (predictions == labels).float().mean()\n",
    "        testing_acc.append(acc.item())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(test_dataloader)}\")\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())   \n",
    "\n",
    "    val_loss = np.mean(testing_loss)\n",
    "    val_acc = np.mean(testing_acc)\n",
    "\n",
    "print(f\"The zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")\n",
    "print(f\"The average testing loss is {val_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/198\n",
      "Batch 20/198\n",
      "Batch 30/198\n",
      "Batch 40/198\n",
      "Batch 50/198\n",
      "Batch 60/198\n",
      "Batch 70/198\n",
      "Batch 80/198\n",
      "Batch 90/198\n",
      "Batch 100/198\n",
      "Batch 110/198\n",
      "Batch 120/198\n",
      "Batch 130/198\n",
      "Batch 140/198\n",
      "Batch 150/198\n",
      "Batch 160/198\n",
      "Batch 170/198\n",
      "Batch 180/198\n",
      "Batch 190/198\n",
      "The zero-shot performance on Food-101 is 77.46%, visual encoder is ViT-B/32.\n",
      "The average testing loss is 0.8691.\n"
     ]
    }
   ],
   "source": [
    "prompt='a photo of a'\n",
    "\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(test_dataloader, start=1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        text_inputs = prompt_encode(prompt)\n",
    "\n",
    "        similarity = model_inference(model, images, text_inputs)\n",
    "        \n",
    "        criterion_CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion_CE(similarity, labels)\n",
    "        testing_loss.append(loss.item())\n",
    "        \n",
    "        predictions = torch.argmax(similarity, dim=-1)\n",
    "        acc = (predictions == labels).float().mean()\n",
    "        testing_acc.append(acc.item())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(test_dataloader)}\")\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())   \n",
    "\n",
    "    val_loss = np.mean(testing_loss)\n",
    "    val_acc = np.mean(testing_acc)\n",
    "    \n",
    "print(f\"The zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")\n",
    "print(f\"The average testing loss is {val_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/198\n",
      "Batch 20/198\n",
      "Batch 30/198\n",
      "Batch 40/198\n",
      "Batch 50/198\n",
      "Batch 60/198\n",
      "Batch 70/198\n",
      "Batch 80/198\n",
      "Batch 90/198\n",
      "Batch 100/198\n",
      "Batch 110/198\n",
      "Batch 120/198\n",
      "Batch 130/198\n",
      "Batch 140/198\n",
      "Batch 150/198\n",
      "Batch 160/198\n",
      "Batch 170/198\n",
      "Batch 180/198\n",
      "Batch 190/198\n",
      "The zero-shot performance on Food-101 is 75.12%, visual encoder is ViT-B/32.\n",
      "The average testing loss is 1.0424.\n"
     ]
    }
   ],
   "source": [
    "prompt='not a photo of a'\n",
    "\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(test_dataloader, start=1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        text_inputs = prompt_encode(prompt)\n",
    "\n",
    "        similarity = model_inference(model, images, text_inputs)\n",
    "        \n",
    "        criterion_CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion_CE(similarity, labels)\n",
    "        testing_loss.append(loss.item())\n",
    "        \n",
    "        predictions = torch.argmax(similarity, dim=-1)\n",
    "        acc = (predictions == labels).float().mean()\n",
    "        testing_acc.append(acc.item())\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(test_dataloader)}\")\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())   \n",
    "\n",
    "    val_loss = np.mean(testing_loss)\n",
    "    val_acc = np.mean(testing_acc)\n",
    "\n",
    "print(f\"The zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")\n",
    "print(f\"The average testing loss is {val_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
